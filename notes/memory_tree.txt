Probe memory banks as a decision tree. Extract relevant sequences.

Let the memory node highway be the same as then standard node highway.

the memories and associations drive the network into areas with different
setups and structures.

create regions of vanishing gradient on purpose. The network is designed to destroy
input in non-relevant paths.

Strong dropout prevents a non-ideal path from contributing to solution.


Allow often taken structural areas to take on broader node thickness.

build layers which resemble the nodes in recurrent neural networks.


Forms of filtering, other than convolutional neural nets....

Filtering is decomposition!

Need filtering which is much more fundamental than temporal ordering.

In NLP:
"What are they talking about?"
"What is their tone"
"What is the writer communicating about themselves?"
"What is the writer communicating about the world?"

Process different queries of the sentance/idea space through different types of
layers.


Randomize and allow an "I'm not sure" response to questions. Punish the "I'm not
sure less than the false positive categorization. The network will align itself to
allocate enough weight to 'i'm not sure' to save loss in murky circumstances.

But while running each training batch, randomly vary all nodes. Add a tiny bit of random
juice to each weight. The batch of slightly varied networks will be compared and the
one with lowest loss considered.

Take the best performer. Only jiggle a bit at a time, so the jiggled networks
have a decent shot of performing near identically.

Pick the best one and perform backpropogation and update.


what are the mathematics of propogation through idea space?

we typically treat differentiable spaces as standard. They appeal to us.

What if the idea space of a document propogates in a differentiable way?

Maybe if you keep the derivatives of the propogation through idea space, you can better
recreate the curve. it helps that the feature space is often converted to continuous.


How are you going to implement these "style weights" and "document memories"?

You thought of weights having three components:
Slow changing structural components, trained on entire corpus.
Mid changing subcorpus weights.
  -Add to structural components.
  -Only train on subcorpus.
  -Don't allow overtraining and overwelming main structure weights.
  -Scale their learning rate to the size of the subcorpus


Maybe networks need to learn about time and causality.

Those arnn't necessarily traits of every universe in which these networks
could be created. We only know about it because of our observed physics.
Have computer differentiate between




Allow more circular thought.

Parse input, filter, encode.
next, re-parse input, this time with encoding as an additional input, allowing
for informed, differentiated decomposition and filtering.
