Step 1: Gather and clean text
    News:
        -Get the API's for NYT and WSJ
        -Reuters
    Encyclopedia:
        -Wikipedia archive
    Literary:
        -Stuff like the Poe archive

Step 2: Word embeddings
    -get set with word2vec or equivalent

Step 3: Setup for multi-task learning
    -synonyms/antonyms
      Probably just word-net
    -annotated text for syntactic parsing
    -annotated text for POS tagging
    -annotated text for NER

Step 4: Train an at least mediocre multi-task NLP NN.
    -Once complete, concatenate effective features onto word embeddings

Step 5: Unsupervised - sentence corruption - max margin
    -Improve sentence encoding

Step 6: Word level text generation:
    -Fill in the blank

Step 7: Sentence level text generation:
    
